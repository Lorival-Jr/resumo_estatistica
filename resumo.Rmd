---
title: "Um resumo do curso de Estatística" 
author: "Lorival Junior"
date: "`r Sys.Date()`"
fontsize: 20pt
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: True
    theme: simplex
linestretch: 1.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

<style>
  body{
    text-align: justify;
    font-size: 16px
    }
  h4 {font-size:20px}
  h5 {font-size:18px}
  strong {fontsize:16px}
</style>

Esse documento busca resumir as matérias do curso de estatística e ser um material de revisão e consulta dessas.
<br>

# **1° Ano**
<p>O primeiro ano é uma base para os próximos anos, sendo constituído de várias matérias de matemática pura e outras de estatística básica, essas matérias serão utilizadas em todos os anos seguintes. Essas matérias sendo, no primeiro semestre, Calculo I, Geometria Analítica, Estatística I e Fundamentos de Probabilidade, já no segundo, Calculo II, Algoritmos, Estatística II e Probabilidade I.</p>
<br>

##  **Cálculo I**

---

<p>Cálculo I é uma matéria fundamental para desenvolvimento do curso, apresentando três novas funções matemáticas, o limite, a derivada e a integração. Essas funções serão utilizadas em todas as matérias tanto de cálculo como de estatística.</p>

### **Limite**

#### **O que é um limite?**
Primeiramente, precisamos entender o que é o limite de uma função.

$$\lim\limits_{x \to 5} f(x) = 4$$

<p>Esse limite seria interpretado como a função f(x) se aproxima de 4 quando x tende a 5. Ou seja, quanto mais o valor de x da função se aproximar de 5, mais o resultado dessa função vai se aproximar de 5. Ou seja, o limite de uma função é o valor que ela se aproxima quando o valor do x se aproxima de outro valor. Assim, o limite de f(x) quando x tende à 5 é 4</p>

<p>Existem outras formas de se aproximar de um número na linha Real, tando pela esquerda, aumentando os número, como 0, 1, 2..., quanto pela direita, diminuindo os números, como 8, 7, 6..., esses são chamados de limites laterais, tendo uma notação para cada na tendência do x. </p> 

$$\lim\limits_{x \to 5^-} f(x) = 4$$
Quando a função se aproxima pela esquerda.

$$\lim\limits_{x \to 5^+} f(x) = 4$$
Quando a função se aproxima pela direita.

<p> Uma função pode ou não ter um limite, se ambos limites laterais forem iguais, ela tem um limete, se não, não existe limite daquela função.</p><br>

#### **Regras operacionais de limite**

$$\lim\limits_{x \to a} f(x) + g(x) = \lim\limits_{x \to a} f(x) + \lim\limits_{x \to a} g(x)$$
Ou seja, o limite da soma de funções de mesma base, é a soma dos limites das funções.

$$\lim\limits_{x \to a} K*f(x) = K*\lim\limits_{x \to a} f(x)$$
Ou seja, o limite de uma constante vezes uma função é o pruto da constante pelo limite

$$\lim\limits_{x \to a} f(x) * g(x) = \lim\limits_{x \to a} f(x) *\lim\limits_{x \to a} g(x)$$
Ou seja, o limite do produto de funções, é o produto dos limites das funções.

$$\lim\limits_{x \to a} \frac{f(x)}{g(x) }= \frac{\lim\limits_{x \to a} f(x)}{ \lim\limits_{x \to a} g(x)}$$
Ou seja, o limite da divisão de funções, é a divisão dos limites das funções.

$$\lim\limits_{x \to a} f(x) ^n= (\lim\limits_{x \to a} f(x))^n$$
Ou seja, o limite de uma função elevada a uma potência, é a potência do limite.
<br>

#### **Calculando Limites**

O limite pode ser calculado via substituição direta, que significa trocar a variável x pelo valor que ela tende, por exemplo:

$$\lim\limits_{x \to 5} x^2 - 1$$

$$5^2 - 1$$

$$25 - 1$$

$$\lim\limits_{x \to 5} x^2 - 1 = 24$$

No entanto, muitas vezes não podemos substituir diretamente como quando o resultado do limite retorna uma indeterminação, por exemplo

$$\lim\limits_{x \to -1} \frac{x^2 -1}{x + 1}$$

$$\frac{-1^2 -1}{-1 + 1}$$

$$\frac{1-1}{-1+1}$$

$$\frac{0}{0}$$
Esse resultado é chamado indeterminação, não podendo ser considerado, para resolver essa indeterminação podemos seguir dois caminhos, o primeiro mexendo na equação e o segundo aplicando L'Hospital.
<br>
**Resolvendo indeterminações**

Olhando a equação anterior podemos separar a equação da parte de cima, o dividendo, é um produto da diferença.
$$\lim\limits_{x \to -1} \frac{x^2 -1}{x + 1}$$
$$\lim\limits_{x \to -1} \frac{(x+1)(x-1)}{x + 1}$$
$$\lim\limits_{x \to -1} (x-1)$$
Agora podemos fazer via substituição direta.

$$(-1 -1)$$
$$\lim\limits_{x \to -1} \frac{x^2 -1}{x + 1} = -2$$

Podemos resolver a inderteminação via L'hospital, que basicamente é derivar a parte de cima da divisão e parte de baixo, independetemente, derivação será explicada mais pra frente.
*OBS: L'Hospital apenas resolver equações que a indeterminação é do tipo $\frac{0}{0}$ e $\frac{\infty}{\infty}$.*

$$\lim\limits_{x \to -1} \frac{(x^2 -1)'}{(x + 1)'}$$
$$\lim\limits_{x \to -1} \frac{(2x)}{(1)}$$
Agora podemos fazer via substituição direta

$$(2*(-1))$$
$$\lim\limits_{x \to -1} \frac{x^2 -1}{x + 1} = -2$$
<br>

####  **Alguns limites fundamentais**

Limites fundamentais são alguns limites que valem um valor conhecido, e são utilizados para descobrir outros limites mais complexos


$$\lim\limits_{x \to 0} \frac{sen(x)}{x} = 1$$
$$\lim\limits_{x \to 0} \frac{1 - cos(x)}{x} = 1$$
$$\lim\limits_{x \to 0^+} \frac{1}{x^r} = +\infty$$
$$\lim\limits_{x \to 0^-} \frac{1}{x^r} =$$ 
Se r é ímpar $-\infty$, se r é par $+\infty$

$$\lim\limits_{x \to -\infty} \frac{1}{x^r} = 0$$ 
$$\lim\limits_{x \to +\infty} \frac{1}{x^r} = 0$$ 
<br>
### **Derivação**

#### **O que é uma derivada?**
A derivada representa o coeficiente de varição da reta tangente da função no ponto, sendo a inclinação dessa reta. Esse processo tem duas principais notações, sendo elas $f'(x)$ e $\frac{d}{dx}f(x)$.
<br>

#### **Regras de derivação**

A derivação de funções mais complexas são resolvidas com base em algumas derivações já conhecidas, essas já conhecidas sendo chamadas de regras de derivação.

**Regra da constante**<br>
A derivada de uma função constante é zero.
Ex:
$$\frac{d}{dx}(5) = 0$$
**Regra da potência**<br>
Conhecida também como regra do tombamento, quando temos uma potência de x descemos a potência como produto e elevamos x potência menos um.
Ex:
$$\frac{d}{dx}(x^5) = 5 \cdot x^{5-1}$$
$$\frac{d}{dx}(x^5) = 5 \cdot x^{4}$$
**Regra da soma**<br>
A derivada de uma soma de funções é a soma das derivadas das funções.
Ex:
$$\frac{d}{dx}(x^3 + 4x) = \frac{d}{dx}(x^3) + \frac{d}{dx}(4x)$$
$$\frac{d}{dx}(x^3 + 4x) = 3x^2 + 4)$$

**Regra do produto**<br>
A derivada de um produto de funções é a soma da derivada da primeira vezes a segunda com a derivada da segunda vezes a primeira.<br>
No caso geral: $(f(x) \cdot g(x))' = f'(x)\cdot g(x) + g'(x)\cdot f(x)$<br>
Ex:
$$(x \cdot (2 - x^2))' = x'\cdot (2 - x^2) + (2 - x^2)'\cdot x$$
$$(x \cdot (2 - x^2))' = 1\cdot (2 - x^2) + (-2x)\cdot x$$
$$(x \cdot (2 - x^2))' = 2 - x^2 -2x^2$$
$$(x \cdot (2 - x^2))' = 2 -3x^2$$

**Regra da Divisão**<br>

A derivada de uma divisão de funções é a diferença da derivada da primeira vezes a segunda com a derivada da segunda vezes a primeira, tudo isso dividido pela segunda ao quadrado.<br>
No caso geral: $\left(\frac{f(x)}{g(x)}\right)' = \frac{f'(x)\cdot g(x) - g'(x)\cdot f(x)}{g(x)^2}$<br>
Ex:
$$\left(\frac{x}{2 - x^2}\right)' = \frac{x'\cdot (2 - x^2) - (2 - x^2)'\cdot x}{(2-x^2)^2}$$
$$\left(\frac{x}{2 - x^2}\right)' = \frac{1\cdot (2 - x^2) - (-2x)\cdot x}{(2-x^2)^2}$$
$$\left(\frac{x}{2 - x^2}\right)' = \frac{1\cdot (2 - x^2) - (-2x)\cdot x}{(2-x^2)^2}$$
$$\left(\frac{x}{2 - x^2}\right)' = \frac{2 - x^2 -2x^2}{(2-x^2)^2}$$
$$\left(\frac{x}{2 - x^2}\right)' = \frac{2 - x^2 -2x^2}{(2-x^2)^2}$$
$$\left(\frac{x}{2 - x^2}\right)' = \frac{2 - 3x^2}{(2-x^2)^2}$$
Essa regra pode ser usada para resolver divisões de derivada, no entanto elas podem ser resolvidas também via regra do produto, lembrando que uma divisão também é um produto<br>
$$\left(\frac{2x- 1}{x}\right)' = [(2x - 1)(x^{-1})]'$$
$$\left(\frac{2x- 1}{x}\right)' = (2x - 1)'\cdot x^{-1} + (x^{-1})'\cdot (2x - 1) $$
$$\left(\frac{2x- 1}{x}\right)' = 2\cdot x^{-1} + (-1\cdot x^{-2})\cdot (2x - 1) $$
$$\left(\frac{2x- 1}{x}\right)' = \frac{2}x - \frac{2x - 1}{x^2} $$
**Regra da cadeia**<br>
A regra da cadeia é usada quando temos uma composta de funções, ela é o produto da derivada da primeira aplicada na segunda com a derivada da segunda.

No caso geral: $(f \circ g)'(x) = f'(g(x))\cdot g'(x) $<br>
Ex:
$$\frac{d}{dx}(x^2 + 3x)^4$$
Nesse exemplo, $f(x) = x^4$ e $g(x) = x^2 + 3x$
Agora é necessário achar f'(g(x)) e g'(x)
$$f'(x) = 4x^3$$
$$f'(g(x))= 4(x^2 + 3x)^3$$
$$g'(x) = 2x + 3$$

$$\frac{d}{dx}(x^2 + 3x)^4 = 4(x^2 + 3x)^3 \cdot (2x + 3)$$
**Derivada da exponencial**<br>
A derivada da exponencial é a derivada do que ela está aplicada multiplicada pela expressão inicial.


**Derivada do seno**<br>
A derivada do seno é a derivada do que seno está aplicado multiplicada pelo cosseno.
<br>
Ex:
$$\frac{d}{dx}e^{2x} = \frac{d}{dx}(2x)\cdot e^{2x}$$
$$\frac{d}{dx}e^{2x} = 2e^{2x}$$
No caso mais comum, a derivada da exponencial de x é exponencial de x, pela derivada de x ser 1.
$$\frac{d}{dx}e^x = e^x$$
**Derivada do cosseno**<br>
A derivada do cosseno é a derivada do que cosseno está aplicado multiplicada por menos seno.
<br>
Ex:
$$\frac{d}{dx}cos(x^2) = \frac{d}{dx}(x^2)\cdot -sen(x^2)$$
$$\frac{d}{dx}cos(x^2) = -2x\cdot sen(x^2)$$
No caso mais comum, a derivada de cossena de x é menos seno de x, pela derivada de x ser 1.
$$\frac{d}{dx}cos(x) = -sen(x)$$


**Regra de L'Hospital**<br>
L'Hospital é usado para resolver indeterminações em limites, facilitando a resolução de indeterminações do tipo $\frac{0}{0}$ e $\frac{+\infty}{+\infty}$. Nessa regra, fazemos a derivada do dividendo e do divisor independentemente, então reaplicamos o limite.
<br>
Ex:
$$\lim\limits_{x \to +\infty} \frac{e^x}{x} $$
Substituindo apenas obteriamos $\frac{e^{+\infty}}{+\infty} = \frac{+\infty}{+\infty}$
<br> Aplicando L'Hospital
$$\lim\limits_{x \to +\infty} \frac{(e^x)'}{(x)'} = \lim\limits_{x \to +\infty} \frac{e^x}{1}$$
Agora Substituindo:

$$\lim\limits_{x \to +\infty} \frac{(e^x)'}{(x)'} = e^{+\infty} = +\infty$$

##### Maximos e mínimos
Uma aplicação de derivadas é para obter máximos e mínimos de funções.

**Pontos críticos**<br>
Todos máximos e mínimos são pontos críticos, mas nem todos pontos crítios são máximos e mínimos, pontos críticos podem ser encontrados substituindo um valor na derivada da função e resultado dá zero ou não existe.

$$f(x) = x^{\frac{4}3} + 4x^{\frac{1}3}$$
$$f'(x) = \frac{4}{3}x^{\frac{1}3} + \frac{4}{3}x^{\frac{-2}{3}}$$
$$f'(x) = \frac{4}{3}\left(x^\frac{1}{3} + \frac{1}{x^\frac{1}3}\right) = \frac{4}{3}x^{\frac{-2}{3}}(x + 1)$$
Se aplicamos em 0 a função não vai existir
$$f'(0) = \frac{4}{3}\left(0^\frac{1}{3} + \frac{1}{0^\frac{2}3}\right)$$
Sendo assim um ponto crítico.
<br> Se aplicarmos em -1
$$f'(-1) = \frac{4}{3}-1^{\frac{-2}{3}}(-1 + 1) = 0$$
Sendo outro ponto crítico<br>

**Regra da derivada primeira**
Os pontos em que a derivada troca de sinal são pontos extremos.

**Regra da derivada segunda**
É usada para determinar se é um ponto de máximo ou mínimo local


### Integração
A integral representa a área, sendo a antiderivada, ou o contrário da derivada.



## Geometria Analítica 

---

## Estatística I

---

<br><br><br>

##  **Fundamentos de probabilidade**

---

### Distribuições Discretas<br><br>
**Distribuição Uniforme Discreta**<br>
Uma das distribuições mais simples, na qual todos pontos tem mesma probabilidade.<br>

 $X$ &nbsp;~&nbsp; $U_{d}(x)$
$$f(x) = p = \frac{1}{n}$$
$\bar{x} = \frac{1}{n}\sum{x}$; &emsp;   $Var(x) = \frac{1}{n}\sum{x^2 - \frac{(\sum{x})^2}{n}}$<br><br><br>
p : A probabilidade de evento acontecer<br>
n : O número de elementos <br><br><br>

**Distribuição Bernoulli**<br>
Nessa distribuição, só há dois resultados possíveis fracasso (0) sucesso (1), e apenas é realizado uma única tentativa de experimento.<br>

$X$ ~ $Ber(p)$<br>
$$f(x) = p^x(1-p)^{1-x}$$
$\bar{x} = p$; &emsp; $Var(x) = p(1-p)$<br>
p : A probabilidade de evento acontecer <br><br><br>

**Distribuição Binomial**<br>
A distribuição binomial é a ocorrência de mais de um experimento de Bernoulli, mantendo x igual a 1 ou 0.<br>
  $X$ &nbsp;~&nbsp; $Binom(n,p)$<br>

$$f(x) = \binom nkp^x(1-p)^{(n-x)}$$
$\bar{x} = n.p$; &emsp; $Var(x) = np(1-p)$<br>
p : A probabilidade de evento acontecer<br>
n : O número de ensaios<br><br><br>

**Distribuição de Poisson**<br>
Nessa distribuição X é uma contagem do número de sucessos em um intervalo.<br>
$X$ ~ $Poisson(\lambda)$<br>

$$f(x) =  \frac{e^{-\lambda}\lambda^x}{x!}$$
$\bar{x} = Var(x) = \lambda$<br>
$\lambda$ : O valor médio de ocorrências.<br><br><br>

**Distribuição Multinomial**<br>
É uma distribuição usada para k eventos mutuamente  exclusivos, com n repetições independetes, tendo uma $p_{i}$ para cada $x_{i}$.<br>
$X$ &nbsp;~&nbsp; $M(x_{1},x_{2},...,x_{k};p_{1},p_{2},...,p_{k})$<br>

$$p(x_{1},x_{2},..,x_{k}) = \frac{n!}{x_{1}!x_{2}!...x_{k}!}p_{1}^{x_{1}}p_{2}^{x_{2}}...p_{k}^{x_{k}}$$
$\bar{x_{i}} = np_{i}$; &emsp; $Var(x_{i}) = np_{i}(1-p_{i})$<br>
$p_{i}$ : A probabilidade do evento $x_{i}$ ocorrer.<br><br><br>

**Distribuição Geométrica**<br>
É uma distribuição usada para encontrar o primeiro sucesso na i-ésima tentativa.<br>
$X$~$G(p)$<br>

$$f(x) = p(1-p)^{x-1}$$
$\bar{x}=\frac{1}{p}$; &emsp; $Var(x)=\frac{1-p}{p^2}$ <br>
p : A probabilidade do evento ocorrer<br><br><br>

**Distribuição Binomial Negativa ou de Pascal**<br>
Nessa distribuição $X$ é o número de experimentos até o r-ésimo sucesso.<br>
$X$&nbsp;~&nbsp;$BN(p,r)$<br>

$$f(x) = \binom{x-1}{r-1}p^r(1-p)^{x-r}$$
$\bar{x} = \frac{r}{p}$; &emsp; $Var(x) = \frac{r(1-p)}{p^2}$<br>
p : A probabilidade do evento ocorrer<br>
r : número de sucessos<br><br><br>

**Distribuição Hipergeométrica**<br>
É usada quando queremos a probabilidade de k sucessos em n retiradas, sem reposição, de uma população de tamanho N que contém x sucessos.<br>

$X$~$H(N,k,n)$<br>
$$f(x) = \frac{\binom k x \binom{N-k}{n-x}}{\binom N n}$$
$\bar{x} = n\frac{k}N = \frac n p$; &emsp; $Var(x)= n \frac k N \left(1-\frac k N \right) \left( \frac{N - n}{N - 1} \right)$<br>
N : Tamanho inicial da população<br>
n : Tamanho selecionado da amostra<br> 
k : Sucessos retirados na amostra<br>
x : Número de sucessos na população<br><br><br><br>

## Calculo II

---

## Algoritmos

---

<br><br><br>

## **Estatística II**

---

### Conceitos iniciais <br><br>

Sendo x uma v.a.c <br><br>

$f(x) \ge 0$ e $\int^{+\infty}_{-\infty}f(x) = 1$ <br><br>

- f é chamada de função densidade de probabilidades (fdp) de x
- FDA: $F(x) = P(X \leq x) = \int^x_{-\infty}f(t)dt$
- $E(x) = \int^{+\infty}_{-\infty}xf(x)dx$
- $Var(x) = E(x^2) - [E(x)]^2$ <br><br>

**Propriedades da média**<br>

- $E(x) = x$
- $E(kx)=kE(x)$
- $E(x \pm y) = E(x) \pm E(y)$
- $E(ax + b) = E(ax) + E(b) = aE(x) + b$ <br><br>

**Propriedades da variância**<br>

- $Var(a) = 0$
- $Var(ax) = a^2Var(x)$
- $Var(x \pm y)=Var(x) + Var(y)$
- $Var(ax + b) = Var(ax) + Var(b) = a^2Var(x)$ <br><br>

### Amostra Aleatória simples

- Todos indivíduos tem a mesma chance de ser selecionado;
- Pode ter ou não reposição, normalmente é sem;
- População é homogênea finita. <br><br>

### Teorema Central do Limite

Se $x_1,...,x_n$ for uma amostra aleatória simples de tamanho n, de média $\mu$ e var finita $\sigma^2$, então a distribuição amostral da média $\bar{x}$ será
<center>$\bar{x}$&nbsp;~&nbsp;$N \left ( \mu, \frac{\sigma^2}{n} \right )$</center>

Representa a distribuição amostral da média. <br><br>

### Estimadores

<p>![](img/estimadores_precisao.png)</p><br>
**Propriedades**<br>

- Não tendencioso (ou não viciado)<br>
&emsp; $E(\hat\theta) = \theta$<br><br>
- Tendencioso (Viciado)<br>
&emsp; Vicio: $B(\hat\theta)=E(\hat\theta)-\theta$<br><br>
- Eficiência 
&emsp;$Var(\hat\theta_1)<Var(\hat\theta_2)$ (menor melhor)

OBS: Se $\hat\theta_1$ é não viciado e tiver variância mínima, dizemos que $\hat\theta$ é ENVVM (Estimador não viciado de Var mínima)<br><br>

### Método de Máxima Verossimilhança

1. $L(\theta) = \prod^n_{i=1} f(x_1,...,x_n|\theta)$ - (Função de Verossimilhança)

2. $l(\theta)=ln(L(\theta))$

3. $\frac{\partial l(\theta)}{\partial\theta}$

4. $\frac{\partial l(\theta)}{\partial\theta}=0$

5. Verificar se $\hat\theta$ encontrado em (4) é ponto de máximo, isso derivando novamente e vendo se é menor que zero.<br><br>

### Estimação intervalar

O que precisamos "saber" para construir um Intervalo de Confiança (IC)?

- amostra
- $1 - \alpha$ (nivel de confiança)
- Distribuição amostral de $\theta$ (ou quantidade pivotal) <br><br>

O intervalo de confiança é constituído de dois intervalos, a área de rejeição e área de aceitação.

- Área de aceitação: É o intervalo em que a hipótese nula é aceita, ficando, frequentemente, em torno da média.
- Área de rejeição: É o intervalo em que a hipótese nula é rejeitada, sendo o intervalo ao entorno do de aceitação.

```{r funcao grafico IC, echo = F}
grafico.teste.norm <- function(alternative = c('greater', 'less', 'two.sided'), z_calc=NULL, conf.level = 0.95)
{
  # Criando os limites baseado no tipo do teste
  if(alternative == 'less')
    {
      lim_inf <- -4
      lim_sup <- qnorm(conf.level)
  }
  
  if(alternative == 'greater')
  {
    lim_inf <- qnorm(1 - conf.level)
    lim_sup <- 4
  }
  
  if(alternative == 'two.sided')
  {
    lim_inf <- qnorm((1 - conf.level)/2)
    lim_sup <- qnorm(conf.level + (1 - conf.level)/2)
  }
  
  # Criando o eixo x
  x <- seq(-4, 4, length=100)
  
  # criando n probabilidades, baseadas no eixo x
  y <- dnorm(x)
  
  data_fill <- data.frame(x = x, y = y)
  data_fill <- data_fill[data_fill$x < lim_sup,]
  data_fill <- data_fill[data_fill$x > lim_inf,]
  
  plot(x,y, type = "l", lwd = 2, axes = FALSE, xlab = "", ylab = "")
  axis(1, at = -3:3, labels = c(seq(-3, 3)))
  polygon(x = c(min(data_fill$x),data_fill$x , max(data_fill$x)),
          y = c(-0.0171, data_fill$y, -0.0171),
          col = "gray", border = T) 
  if (!is.null(z_calc))
  {abline(v = z_calc, lwd = 2, col = 'red', lty = 'dashed')
  text(z_calc, 0, paste0('Zc=',z_calc), col='black', cex = 0.8)}
}

grafico.teste.norm('two.sided', conf.level = 0.80)
```
<p>Esse seria um exemplo do gráfico do intervalo de confiança para uma normal, a área cinza a área de aceitação em torno da média que possui área $1 - \alpha$, e área branca a área de rejeição, possuindo área $\alpha$.</p>

Os intervalos podem ser representados das seguintes formas

$$P\left( \bar{x} - Z_{\frac{\alpha}{2}}\cdot \frac{\sigma}{\sqrt n}\leq \mu \leq \bar{x} + Z_{\frac \alpha 2} \cdot \frac{\sigma}{\sqrt n} \right) = 1 - \alpha$$
<center> **Ou** </center>

$$IC(\mu, 1 - \alpha) = \bar{x} \pm Z_{\frac{\alpha}{2}}\cdot \frac{\sigma}{\sqrt n}$$
<br><br>

#### Intervalo de confiança para $\mu$ quando $\sigma$ for desconhecido

**a) Amostras grandes**

- $X$ &nbsp;~&nbsp; $N(\mu,\sigma^2)$
- $\bar{X}$ &nbsp;~&nbsp; $N(\mu,\frac{\sigma^2}{n})$
- $IC(\mu, 1 - \alpha)$ = $\bar{x} \pm Z_{\frac{\alpha}{2}}\cdot \frac{S}{\sqrt n}$<br><br>

**b)Amostra pequena**

- $IC(\mu, 1 - \alpha)$ = $\bar{x} \pm t_{\left (\frac{\alpha}{2},n-1 \right )}\cdot \frac{S}{\sqrt n}$<br><br>

Em que t é a distribuição t student com n-1 graus de liberdade<br><br>

$$f(x) = \frac {\Gamma \left ( \frac{k + 1}{2} \right )}{\sqrt{\pi k} \Gamma{ \left ( \frac{k} {2} \right )}} \frac{1}{\left ( \frac{x^2}{k} + 1 \right ) \frac{k +1}2}$$<br><br>

#### Intervalo de confiança para a proporção populacional

- $p$ <- proporção populacional
- $\hat p$ <- proporção amostral
- $\hat p$ &nbsp;~&nbsp; $N\left (p, \frac{p(1-p)}n \right )$

Para esse intervalo de confiança há duas formas de constuílo, a otimista e a conservadora:<br><br>

$IC(p, 1-\alpha) = \hat p \pm Z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{\hat p (1 - \hat p)} n}$ (otimista)

$IC(p, 1-\alpha)=\hat p \pm Z_{\frac{\alpha}{2}} \cdot \sqrt{\frac 1 {4n}}$ (conservador)<br><br>

**Margem de Erro**<br>

- $e = Z_{\frac{\alpha}{2}} \sqrt{\frac{\hat{p}(1- \hat p)}{n}}$ (Otimista)
- $e = Z_{\frac{\alpha}{2}}\sqrt{\frac{1}{4n}}$<br><br>

**Tamanho da amostra**<br>

- $n = \frac{Z_{\frac{\alpha}{2}}^2 \hat p  (1- \hat p)}{e^2}$ (Otimista)
- $n = \left (Z_{\frac{\alpha}{2}} \frac{1}{2e}\right )^2$
- $n = \left ( Z_{\frac{\alpha}{2}} \frac \sigma e \right )^2$
- $n = \left (  t_{\left ( \frac{\alpha}{2},n-1 \right )} \cdot \frac S e \right )^2$<br><br>

#### Erros associados a testes de hipóteses

```{r Erros testes hipoteses, echo= F}
erros_t12 <- data.frame('Decisão' = c('Rejeita H0', 'Não Rejeita H0'), 'H0 verdadeira' = c('Erro Tipo I', 'Sem erro'), 'H0 Falsa' = c('Sem erro', 'erro tipo II'))

head(erros_t12)
```
<br><br>
Para que serve um teste de hipóteses?

&emsp; Auxiliar na tomada de decisão com base em valores amostrais

O que é um teste de hipótese?

&emsp; Uma metodologia usada para chegar na tomada de decisão
<br><br>

- Procedimento:
1. Especificar as hipóteses nulas e alternativas<br>
2. Estabelecer a estatística teste e sua distribuição<br>
3. Estabelecer o nível de significância e regiçoes críticas<br>
4. Calcular a estatística teste<br>
5. Tomar a decisão<br><br>

Exemplo hipótese:<br>
$H_0: \theta = \theta_0$<br>
$H_1: \theta \neq \theta_0$

Sempre $H_0$ sempre carrega o igual sendo $=,\ge,\le$.<br><br>

#### Teste de hipótese proporção<br><br>

$Zc = \frac{\hat p - p}{\sqrt{\frac{p(1-p)}{n}}}$ &nbsp;~&nbsp; $N(0,1)$<br><br>

#### Cálculo do valor p:

Como o p-valor funciona?<br><br>

Basicamente se o p-valor for menor que a significância rejeita-se $H_0$ 

(Pode ser aplicado a $t_c$, $\chi^2_c$, $F_c$, etc)<br>
- Se $H_1: \theta \neq \theta_0$ => $p = 2(1- \Phi(z_c))$ <br>
- Se $H_1: \theta > \theta_0$ => $p = 1- \Phi(z_c)$ <br>
- Se $H_1: \theta < \theta_0$ => $p = \Phi(z_c)$ <br><br><br>

#### Poder do teste e tamanho da amostra

**Se bilateral**

$$n = \left ( \frac{Z_{\frac{\alpha}{2}} \sqrt{\hat p(1- \hat p)} + Z_\beta \sqrt{p(1-p)}}{p - \hat p} \right )^2$$

**Se unilateral**

$$n = \left ( \frac{Z_{\alpha} \sqrt{\hat p(1- \hat p)} + Z_\beta \sqrt{p(1-p)}}{p - \hat p} \right )^2$$<br><br><br>

#### Teste de hipótese para $\mu$

**1) Se $\sigma$ for conhecido, então**

- ##### $z_c = \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$ &nbsp;~&nbsp; $N(0,1)$

**2) Se $\sigma$ for desconhecido e a amostra pequena, então**

- ##### $t_c = \frac{\bar{x} - \mu}{\frac{S}{\sqrt{n}}}$  &nbsp;~&nbsp; $t_{n-1}$

**2) Se $\sigma$ for desconhecido e a amostra grande, então**

- ##### $z_c = \frac{\bar{x} - \mu}{\frac{S}{\sqrt{n}}}$ &nbsp;~&nbsp; $N(0,1)$
<br><br>

#### Distribuição Qui-quadrado <br><br>

$x$ &nbsp;~&nbsp; $\chi^2_k$

$$f(x;k)= \frac{1}{\Gamma \left(\frac{k}2 \right )2^{\frac{k}{2}}}x^{\frac{k}2 -1}e^{\frac{-x}2} $$
<br>

```{r qui-quadrado, echo= FALSE}

set.seed(1)

hist(rchisq(100000, 4),
     main = 'Distribuição Qui-quadrado com k=4',
     xlab= 'x',
     ylab = 'Probabilidade', 
     breaks = 30,
     freq = F,
     ylim = c(0,0.25),
     xlim = c(0, 20),
     col = 'gray25'
     )

```

#### Teste de hipóstese para $\sigma^2$ <br><br>

Se $Z_i$ &nbsp;~&nbsp; $N(0,1)$ <br><br>

<center> $\sum^n_{i=1}(z_i-\bar{z})^2$ &nbsp;~&nbsp; $\chi^2_{n-1}$ </center><br>

<center> e assim </center><br>

<center> $\chi^2_c = \frac{(n-1)S^2}{\sigma^2}$ &nbsp;~&nbsp; $\chi^2_{n-1}$ </center> <br><br><br>

#### Teste de hipótese para variância populacional (Fischer)

<center> $F_c = \frac{s_1^2}{s_2^2}$ &nbsp;~&nbsp; $F_{n_1 -1, n_2 - 1}$ </center>
<br><br>

- Depende de duas amostras, e $s_i$ é a variância de cada amostra
- $s_1^2$ sendo sempre maior que $s_2^2$
- É um teste para verificar se duas variâncias populacionais são diferentes baseado nas amostrais
- Por padrão o raio da diferença é 4, mas pode ser ajustado dependendo da situação <br><br>

Se $F_c \ge4$, então $\sigma^2_1 \neq \sigma^2_2$

Se $F_c <4$, então $\sigma^2_1 =\sigma^2_2$ <br><br>

#### Teste de hipóteses para $\mu_1 - \mu_2$ (Diferença de médias)

1. Se $\sigma_1$ e $\sigma_2$ são conhecidos, então

&emsp;$z_c = \frac{\bar{x_1}-\bar{x_2}}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}}$ &nbsp;~&nbsp; $N(0,1)$<br><br>

2. Se $\sigma_1$ e $\sigma_2$ são desconhecidos e iguais, então

&emsp;$t_c = \frac{\bar{x_1}-\bar{x_2}}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$ &nbsp;~&nbsp; $t_{(n1 + n2 -2)}$

em que

&emsp;$s_p = \sqrt{\frac{(n_1-1)s_1^2 +(n_2 - 1)s_2^2}{n_1 + n_2 - 2}}$<br><br>

3. Se $\sigma_1$ e $\sigma_2$ são desconhecidos e diferentes, então

&emsp;$t_c = \frac{\bar{x_1}-\bar{x_2}}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$ &nbsp;~&nbsp; $t_{\phi}$

em que

&emsp;$\phi = \frac{(\omega_1 + \omega_2)^2}{\frac{\omega_1^2}{n1-1} + \frac{\omega_2^2}{n2-1}}$;&emsp;
$\omega_1 = \frac{s^2_1}{n_1}$ &emsp;e&emsp; $\omega_2 = \frac{s^2_2}{n_2}$<br><br><br>

#### Teste t para dados pareados

$t_c = \frac{\bar{d}}{\frac{s_d}{\sqrt{n}}}$

em que,

&emsp; $d$ = Depois - Antes

&emsp; $\bar{d}$ é a média das diferenças

&emsp; $s_d$ é o desvio padrão das diferenças <br><br>

Ex: Verificar se houve mudanças na pressão arterial cistólica após o uso de determinbado farmaco.

|Antes| 139 | 116 | 136 | 133 | 132 | 131 | 124 | 135 | 133 | 144 |121|115|
|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
|Depois| 141 | 113 | 134 | 136 | 133 | 133 | 128 | 134 | 137 | 148|128|118|
<br>
<br>
Diferenças = {2,-3,-2,3,1,2,4,-1,4,4,7,3}
<br><br>
$\bar{d} = 2$

$S_d = 2.860$

$n = 12$

$\alpha = 0.05$

$H_0: \mu_D - \mu_A = 0$

$H_1: \mu_D - \mu_A \neq 0$

$t_c = \frac{\bar{d}}{\frac{s_d}{\sqrt{n}}} = \frac{2}{\frac{2.860}{\sqrt{12}}}=2.422$

```{r teste hipotese t pareado,echo = F}

grafico.teste.norm('two.sided', 2.422,)

```
<p>Assim, com 0.95 de confiança, a estatística $t_c$ é igual 2.422, estando na região de rejeição, portanto rejeita-se $H_0$, apresentando evidências que há diferenças de pressão antes e depois da aplicação do fármaco.</p><br>

#### Teste Qui-Quadrado

- Utilizado para avaliar a associção existente entre variáveis qualitativas

- É utilizado para verificar se variáveis estão associadas, se amostras diferentes em uma série de experimentos semelhantes são homogêneos e testar a adequalidade de um modelo probabilístico a um conjunto de dados.

$\chi = \sum\frac{(\theta - e)^2} e$ &nbsp;~&nbsp; $\chi^2_m$
<br><br>
em que
<br><br>
$m = (r-1)(s-1)$<br><br>

$\theta$: é a frequência obeservada 

$e$: é a frequência esperada <br><br>

Como calcular o esperado?

$e$ = $\frac{marginal(x)\cdot marginal(y)}{total(n)}$<br><br>

Como tomar a decisão?

$\chi^2_c \ge \chi_t^2$ $\rightarrow$ Rejeitar $H_0$

$\chi^2_c < \chi_t^2$ $\rightarrow$ Não rejeitar $H_0$<br><br>

**Pressupostos**

- Os grupos devem ser independentes e devem ser selecionados aleatoriamente

- As observações devem ser frequências ou contagens

- Cada observação pertence a uma e somente uma categoria

- A amostra deve ser relativamente grande<br><br>

Ex: Estamos interessados em estudar se a queixa de dores (presentes ou ausente) em idosos está associada a participação em um programa de atividade física. Para isso selecionou-se uma amostra de 150 idosos e foi construido a seguinte tabela de contingência:


| Queixas | Participou | Não participou | Total |
|---------|------------|----------------|-------|
| Presente| 55         | 21             | 76    |
| Ausente | 42         | 32             | 74    |
| Total   | 97         | 53             | 150   |

$H_0:$ A queixa de dores e o programa são independentes

$H_1:$ A queixa de dores e o programa não são independentes

$\alpha = 0.05$<br><br>

esperado1 = $\frac{76 \cdot97}{150} = 49.1466 \approx 49.15$

esperado2 = $\frac{76 \cdot53}{150} = 49.3333 \approx 26.85$

esperado3 = $\frac{74 \cdot97}{150} = 49.1466 \approx 47.85$

esperado4 = $\frac{74 \cdot53}{150} = 49.1466 \approx 26.15$

$\chi^2_c = \frac{(55 - 49.15)^2}{49.15} + \frac{(42 - 47.85)^2}{47.85} + \frac{(21 - 26.85)^2}{26.85} + \frac{(32 - 26.15)^2}{26.15}$

$\chi^2_c  = 3.9945$

$\chi^2_t = \chi^2_{(r -1)(s-1); 0.05} = \chi^2_{(2 -1)(2-1); 0.05} = \chi^2_{1; 0.05} = 3.841$<br><br>

Como $\chi^2_c>\chi_t^2$, então rejeitamos $H_0$, ou seja, com 95% de confiança, existem indícios de que dores e o programa não são independentes.<br><br><br>

#### Regressão linear simples

1. Correlação

2. Modelo de regressão

3. Diagnóstico

4. Extrapolação <br><br>

**Correlação**

**Coeficiente de correlação linear de Pearson**

$$\rho = \frac{cov(x,y)}{s(x)s(y)}$$

$$\rho = \frac{n\sum x_i y_i - \sum x_i \sum y_i}{\sqrt{n \sum_i  x_i^2 - (\sum_i x_i)^2} \cdot \sqrt{n \sum_i y_i^2 - (\sum_i y_i)^2}}$$

OBS: $-1 \le \rho \le 1$<br><br>

##### Teste de significância para $\rho$

$H_0:$ $\rho = 0$

$H_1:$ $\rho \neq 0$

$t_c = r \sqrt{\frac{n-2}{1-r^2}}$ &nbsp;~&nbsp; $t_{n-2}$<br><br>

Conclusão:

$|t_c| \le |t_t| \rightarrow$ não rejeita $H_0$

$|t_c| > |t_t| \rightarrow$ rejeita $H_0$<br><br><br>

##### Modelo de regressão

Esse modelo por ser linear segue a função

$\hat{a} = \bar{y} - \hat{b}\bar{x}$

$\hat{b} = \frac{n\sum x_i y_i - \sum x_i \sum y_i}{n \sum x_i^2 - (\sum x_i)^2}$<br><br><br>

##### Resíduos

$r_i = y_i - \hat{y_i}$

$\hat{\sigma^2} = \frac{S_{aE}}{n-2}$

em que

$S_{aE} = \sum_i(y_i - \hat{y_i})^2$<br><br><br>

##### Inferência sobre os parâmetros do modelo

a) Para o coeficiente angular ($\hat{b}$)

&emsp;$E_p(\hat{b}) = \sqrt{\frac{\sigma^2}{S_{xx}}}$

&emsp;$\hat{b} \pm \sqrt{\frac{\sigma^2}{S_{xx}}}t_{(n-2); \frac \alpha 2}$

&emsp;$t_c = \frac{\hat{b} - b}{\sqrt{\frac{\hat{\sigma^2}}{S_{xx}}}}$ &nbsp;~&nbsp; $t_{n-2}$

&emsp;$H_0: \hat b = 0$

&emsp;$H_1: \hat b \neq 0$

&emsp;Se |$t_c$| > $t_{\frac{\alpha}{2}; n-2} \rightarrow$ rejeita $H_0$
<br><br>

b) Para o intercepto ($a$)

&emsp;$EP(\hat{a}) = \sqrt{\hat \sigma^2 \left (\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}} \right)}$

&emsp;$\hat{a} \pm \sqrt{\hat \sigma^2 \left (\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}} \right )} t_{\frac \alpha 2 ; n-2}$

&emsp;$H_0: a = a_0$

&emsp;$H_1: a \neq a_0$

&emsp;$t_c = \frac{\hat a - a}{\sqrt{\hat \sigma^2 \left (\frac{1}{n} + \frac{\bar{x}^2}{ S_{xx}}\right )}}$

Se $|t_c| > t_{\frac \alpha 2 ; n-2}$ Rejeita-se $H_0$<br><br>

## **Probabilidade I**

---

### Distribuições Contínuas<br><br>

#### **Conceitos Gerais**<br><br>

#### **Função de Distribuição de Probabilidades - FDP**

Uma f(x) é uma função densidade de probabilidade (fdp) da variável x e possui um suporte no qual a probabilidade é maior que 0.

$$P(a \leq x \leq b) = \int_{a}^{b}f(x)dx$$

Propriedades:

- $f(x) \geq 0, \forall x \in I\!R$ 
- $\int_{-\infty}^{+\infty}f(x)dx=1$
- $P(x=0)=\int_{a}^{a}f(x)dx=0, a \in I\!R$
- $P(a\leq x\leq b) = P(a\leq x< b) = P(a<x\leq b) = P(a<x<b)$
- É a derivada da fda.


&emsp;Toda função que satisfaça as duas primeiras propridades é uma fdp.<br><br>

#### **Função de Distribuição Acumulada - FDA**

A Função de Distribuição Acumulada (fda) é a probabilidade do evento estar em um intervalo, sendo a integral da fdp.

$$F(x)=P(x \leq X) = \int_{-\infty}^{x}f(t)dt, \forall x \in I\!R$$

- $F(x)$ é não decrescente;
- $F(-\infty) = 0$ e $F(+\infty)=1$
- $F(x)$ é contínua a direita<br><br><br>


#### **Distribuição Normal**<br><br>
$Z$&nbsp;~&nbsp;$N(\mu,\sigma)$
$$f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}$$
$$F(x) = \int\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}dx$$
Escopo:<br>
$-\infty < x < +\infty$, $-\infty < \mu < +\infty$, $\sigma > 0$<br><br>

**Propriedades**<br>

- $E(x) = \mu$
- $f(x) \ge 0, \forall x$
- A curva é simétrica em relação a $\mu$
- É assintótica em relação ao eixo x.
- O ponto de máximo é $\left ( \mu, \frac{1}{\sigma \sqrt{2 \pi}} \right )$

```{r Normal, echo = FALSE}
set.seed(1)

hist(rnorm(100000),
     main = 'Distribuição Normal',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,0.45),
     col = 'gray25'
     )


```
<p>**Padronizando a normal**</p>
Relembrando que uma Normal padrão é $Z$~$N(0,1)$, então se $Z$~$N(\mu,\sigma)$:<br>

$$Z = \frac{x - \mu}{\sigma}$$
<center>**e**</center> 
$$f(z)=\int^z_{-\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}2}$$<br><br><br>

#### **Distribuição Exponencial**<br>
$X$~$exp(\theta)$
$$f(x) = \theta e^{-\theta x}$$
$$F(x) = 1 - e^{-\theta x}$$
Escopo:<br>
$0 < x < +\infty$, $\theta > 0$<br><br>

**Propriedades**<br>
$E(x^1) = \frac{1}{\theta}$, $E(x^r) = \frac{r}{\theta}E(x^{r-1})$, $Var(x) = \frac{1}{\theta^2}$

```{r exponencial, echo = FALSE}
set.seed(1)

hist(rexp(100000),
     main = 'Distribuição Exponencial',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,0.45),
     col = 'gray25'
     )


```
<br><br>


#### **Distribuição Gama**

$X$~$Gama(\alpha,\beta)$

$$f(x) = \frac{\beta^\alpha x^{\alpha -1}e^{-\beta x}}{\Gamma(\alpha)}$$
Sendo, &emsp; $\Gamma(\alpha) = \int_0^\infty x^{\alpha -1}e^{-x}dx$

Escopo:<br>
$\alpha > 0$, $\beta > 0$ <br> <br>

**Propriedades**<br>
$\bullet$  $E(x) = \frac{\alpha} \beta$, $Var(x) = \frac{\alpha}{\beta^2}$, $E(x^2) = \frac{\alpha (\alpha + 1)}{\beta^2}$<br>
$\bullet$  $\Gamma(n) = (n -1)!$ (n inteiro)<br>
$\bullet$ $\Gamma(y + 2) = (y + 1)\Gamma(y + 1) = y(y+1)\Gamma(y)$<br>
$\bullet$  $\Gamma(\frac{1}{2}) = \sqrt \pi$<br>
$\bullet$  $X$ ~ $Gama(1,\beta)  = X$ ~ $Exp(\beta)$<br>
$\bullet$ $X$ ~ $Gama(\frac{1}2, \frac{n}{2}) = X$ ~ $\chi_n^2$


```{r Gama, echo = FALSE}
set.seed(1)

hist(rgamma(100000, 3),
     main = 'Distribuição Gama',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,0.3),
     col = 'gray25'
     )
```
<br><br>

#### **Distribuição Beta**<br>

$$f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}x^{\alpha -1}(1-x)^{\beta-1}$$
<br><br>
```{r beta, echo = FALSE}
set.seed(1)
par(mfrow = c(2,3))
hist(rbeta(100000, 2, 2),
     main = 'Distribuição Gama(2,2)',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,2),
     col = 'gray25'
     )

hist(rbeta(100000, 5, 2),
     main = 'Distribuição Gama(5,2)',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,3),
     col = 'gray25'
     )

hist(rbeta(100000, 2, 5),
     main = 'Distribuição Gama(2,5)',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,3),
     col = 'gray25'
     )

hist(rbeta(100000, 1, 5),
     main = 'Distribuição Gama(1,5)',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,5),
     col = 'gray25'
     )

hist(rbeta(100000, 5, 1),
     main = 'Distribuição Gama(5,1)',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,5),
     col = 'gray25'
     )

hist(rbeta(100000, 0.5, 0.5),
     main = 'Distribuição Gama(0.5, 0.5)',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,3),
     col = 'gray25'
     )

```

Escopo:<br>
$0 < x < 1$, $\alpha > 0$, $\beta > 0$<br><br>

**Propriedades**<br>
 $E(x) = \frac{\alpha}{\alpha + \beta}$, 
 $Var(x) = \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}$ <br>

- $\alpha$, $\beta$ são parâmetros de forma
- Se $\alpha$ = $\beta$ então $f(x)$ é simétrico e $E(x) = 0.5$
- Se $X$ ~ $Beta(1,1) = X$ ~ $Unif(0,1)$<br><br><br>

#### Distribuição Uniforme Contínua

$X$ ~ $Unif(a, b)$

$$f(x) = \frac{1}{b-a}$$
$$F(x) = \frac{x-a}{b-a}$$

Escopo:<br>
$a,b \in \rm I\!R$

```{r Unif, echo = FALSE}
set.seed(1)
par(mfrow = c(1,1))

hist(runif(100000),
     main = 'Distribuição Uniforme',
     xlab= 'x',
     ylab = 'Probabilidade', 
     freq = F,
     ylim = c(0,1.2),
     col = 'gray25'
     )
```

**Propriedades**<br>
$E(x) = \frac{a + b}{2}$, $Var(x) = \frac{(b - a)^2}{12}$<br><br><br>

### Variáveis Bidimensionais<br><br>

#### Covariância

É uma medida de associação entre duas v.a's x e y.

$$Cov(x,y) = E(xy) - E(x)E(y)$$

#### Correlação

A Corr(x,y) varia entre -1 e 1, quanto mais próximo de zero for a correlação, menor é a associação entre x e y.

$$Corr(x,y)=\frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}$$

Propriedades

- $Cov(x,y)=Cov(y,x)$, a orderm não importa
- $Cov(ax,by)=abCov(x,y)$
- $Cov(x,y) = Corr(x,y) = 0$, se x e y são independentes
- $E(ax + by) = aE(x) + bE(y)$
- $Var(x \pm y) = Var(x) + Var(y) \pm 2Cov(x,y)$
- $Var(ax + by) =a^2Var(x) + b^2Var(y) + 2abCov(x,y)$
- $E(xy) = E(x)E(y)$, se x e y são independentes<br><br><br>

#### Distribuição condicional

$$P(x = x|Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}$$<br><br>
Se x e y forem independentes<br><br>

$$P(X=x|Y=y) = \frac{P(X=x) P(Y=y)}{P(Y=y)} = P(x = x)$$
<br><br><br>

#### Transformação de variáveis

Seja X uma v.a. discreta com fdp P(X=x) defina y = g(x), a fdp de Y é

$$I\!P(Y=y)=I\!P(g(x)=y)=\sum I\!P(X=x)$$
Ex:

| x      | -3  | -2  | -1  | 0   | 1   | 2   | 3   |
|--------|-----|-----|-----|-----|-----|-----|-----|
| P(X=x) | 1/7 | 1/7 | 1/7 | 1/7 | 1/7 | 1/7 | 1/7 |
<br><br>
**Encontre a fdp de Y=x²-x**<br><br>


| x         | -3 | -2 | -1 | 0 | 1 | 2 | 3 |
|-----------|----|----|----|---|---|---|---|
| Y=x^2 - x | 12 | 6  | 2  | 0 | 0 | 2 | 6 |
<br><br>
**Assim,**<br><br>

| y      | 0   | 2   | 6   | 12  |
|--------|-----|-----|-----|-----|
| P(Y=y) | 2/7 | 2/7 | 2/7 | 1/7 |

<br><br><br>


### Função Geradora de Momentos - FGM

Se X for discreto,
$$M_X(t) = E(e^{tx}) = \sum_x e^{tx}I\!P(X=x)$$
Se X for contínuo,
$$M_X(t) = E(e^{tx}) = \int^{+\infty}_{-\infty}e^{tx}f(x)dx$$

O r-ésimo momento de um v.a. X pode ser calculado derivando a fgm r vevez e avaliando no ponto t=0

$$E(x^r)=\left. \frac{d^r}{d^rt}m(t)\right|_{t=0} = m^{(r)}(0)$$
Propriedades:

- $Var(x) = m''(0) - (m'(0))^2$
- $X_1, X_2, ..., X_n$, n v.a's i.i.d. então $M_{x_1 + x_2 + ... + x_n}(t) = \prod_{i=1}^n m_{x_i}(t) = (m_{x_i}(t))^n$
- Unicidade: Se X e Y tem a mesma fgm então X e Y são identicamente distribuídas.
- Seja $Y = ax + b$ sua fgm será $M_Y(t) = M_{ax + b}(t) = e^{bt}m_x(at)$

<br><br>

# 2° Ano

## Calculo III

## Algebra Linear

## Estrutura de Dados

## **Probabilidade II**

---

### Função de distribuição acumulada conjunta<br><br>

Sejam X e Y duas v.a's.</br>
A função de distribuição acumulada conjunta de X e Y é dada por: <br><br>

$F(a,b) = F_{xy} = I\!P(X \le a, Y \le b)$
<br><br>
A fda de X pode ser obtida a partir da conjunta

$F_x(a) = I\!P(X \le a) = \int_{-\infty}^{a} f(x,y)dy$<br>
$F_y(b) = I\!P(X \le b) = \int_{-\infty}^{b} f(x,y)dx$<br>

AS funções $F_x$ e $F_y$ são chamadas de distribuição (ou fda) marginal de X e Y.<br><br>

**Propriedades**

-  $I\!P(X>a, Y>b) = 1 - F_x(a) - F_y(b) + F(a,b)$

-  $I\!P(a_1 < x \le a_2, b_1 < y \le b_2) = F(a_1, b_1) + F(a_2, b_2) - F(a_1,b_2) - F(a_2,b_1)$

- $\lim\limits_{a,b \to -\infty} F(a,b) = 0$
&emsp; $I\!P$ insignificantes são consideradas zero
<br><br><br>

### Independência de V.a's<br><br>

As variáveis X e Y são independentes se $I\!P(X \in A, Y \in B) = I\!P(X \in A)I\!P(Y \in B)$ <br><br>

X e Y são independentes ($X \perp Y$) se e somente se $F(a,b) = F_x(a)F_y(b)$ , $\forall a,b$<br><br><br>

### Distribuição condicional <br><br>

A fdp condicional de x, dado que o evento {Y=y} ocorreu é definido por<br><br>

$$f_{x|y}(x|y)=\frac{f(x,y)}{f_y(y)},x \in I\!R$$
E a fda condicional é<br><br>

$$F_{x|y}(a|y)= I\!P(x \le a|Y=y)= \int^a_{-\infty} f_{x|y}(x|y)dx$$
<br>
**Propriedades**

- $f(x,y) = f_{x|y}f_y(y) = f_{y|x}(y|x)f_x(x)$

- Se X e Y são independentes temos $f_{x|y}(x|y) = \frac{f(x,y)}{f_y(y)}=\frac{f_x(x)f_y(y)}{f_y(y)} = f_x(x)$<br><br><br>

### Soma de v.a's independentes <br><br>

A fdp de X + Y é obtida como 

$$f_{x+y}(a)= \int^{+\infty}_{-\infty} f_{x}(a-y)f_y(y)dy$$

A fda de X + Y é

$$\int^{+\infty}_{-\infty}F_x(a-y)f_y(y)dy$$
<br><br><br>

### Covariância e Correlação<br><br>

- $E(x)=\int \int x f(x,y)dydx =\int x \int f(x,y)dydx =\int x f_x(x)dx$

- $E(X+Y) = \int \int (x+y)f(x,y)dxdy = \int\int xf(x,y)dxdy + \int\int yf(x,y)dxdy$<br>
$E(X+Y)=\int x f_x(x)dx + \int y f_y(y)dy = E(X) + E(Y)$<br>
$E(X + Y) = E(X) + E(Y)$

- Se $X \perp Y$, então $E(g(x)h(x)) = I\!E(g(x))I\!E(h(y))$, em particular, $I\!E(XY) = I\!E(X)I\!E(Y)$<br><br>

**Covariância** entre X e Y é dado por $Cov(X,Y) = E(XY) - E(X)E(Y)$

- $Cov(X,Y) = Cov(Y,X)$

- $Cov(X,X) = Var(X)$

- $Cov(aX, bY) = ab\cdot Cov(X,Y)$

- $Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$

- $Var(X-Y) = Var(X) + Var(Y) - 2Cov(X,Y)$<br><br>

**Correlação** entre X e Y é dado por $Corr(x,y)=\frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}} = \frac{E(XY) - E(X)E(Y)}{\sqrt{Var(x)Var(y)}}$

- $-1\le Corr(X,Y) \le 1$

- $Corr(aX,bY) = Corr(X,Y)$<br><br><br>

### Normal Bivariada<br><br>

$f(x,y) = \frac{1}{2\pi\sigma_x\sigma_y\sqrt{1 - \rho^2}}e^{\frac{-1}{2(1-\rho^2)}[\left (\frac{x-\mu_x}{\sigma_x}\right )^2 -2\rho \left (\frac{y-\mu_y}{\sigma_y}  \right ) + \left (\frac{y + \mu_y}{\sigma_y} \right)^2]}$

$(x,y) \in I\!R^2, (\mu_x,\mu_y) \in I\!R^2, \sigma_x, \sigma_y > 0, -1 \le \rho \le 1$<br><br>

### Máximos e Mínimos<br><br>

**Máximo**

$g_n(x) = n(F(x))^{n-1}$

$G_n(x) = (F(x))^n$

**Mínimo**

$g_1(x) = n(1 - F(x))^{n-1}f(x)$

$G_1(x) = 1 - (1 -F(x))^n$<br><br>

A fdp conjunta de ($x_{(1)},x_{(n)}$) é <br><br>

$f_{x_1,x_n}(x_1,x_n) = n(n-1)(F(x_n)-F(x_1))^{n-2}f(x_1)f(x_n), x_n \ge x_1$<br><br>

A densidade marginal da j-ésima estatística de ordem é

$f_{x_{j}}(x) = \frac{n!}{(n-j)!(j-1)!}(F(x))^{j-1}(1-F(x))^{n-j}f(x)$
<br><br><br>

### Funções de Vetores Aleatórios<br><br>

Seja ($X_1, X_2$) uma v.a com f.d.p conjunta f($x_1, x_2$). Suponha que estamos interessados nas v.a's

$Y_1 = g_1(x_1, x_2)$

$Y_2 = g_2(x_1, x_2)$<br><br>

Suponha que:

1. As funções inversas existam e sejam únicas

&emsp; $x_1 = h_1(y_1, y_2)$

&emsp; $x_2 = h_1(y_1, y_2)$
<br><br>

2. $g_1$ e $g_2$ têm derivadas parciais contínuas e Jacobiano (J)

&emsp; $J(x_1,x_2) = \left | \frac{\partial(g_1, g_2)}{\partial (x_1,x_2)} \right | =  \left | \frac{\partial g_1}{\partial x_1} \right | = \begin{vmatrix}
\frac{\partial g_1}{\partial x_1}&\frac{\partial g_1}{\partial x_2}\\
\frac{\partial g_2}{\partial x_1}&\frac{\partial g_2}{\partial x_2}\\
\end{vmatrix} \neq 0$

Então, a fdp da v.a ($Y_1, Y_2$) é

$$f_{Y_1, Y_2}(y_1, y_2) = f_{X_1, X_2}(x_1, x_2) |J(x_1, x_2)|^{-1}$$
Sendo $x_1 = h_1(y_1, y_2)$ e $x_2 = h_2(y_1, y_2)$

Ex: Sejam $X_1, X_2$ v.a's independentes com distribuição exponencial do parâmetro $\lambda_1, \lambda_2$, respectivamente, determine a fdp conjunta e sejam:

$$Y_1 = x_1 + x_2$$
$$Y_2 = x_1 - x_2$$
<br><br>
Resolvendo:

colocando $x_1$ e $x_2$ em função de $Y_1$ e $Y_2$

$Y_1 = x_1 + x_2$, &emsp;  $x_1 = Y_1  - x_2$

Substituindo $x_1$ em $Y_2$

$Y_2 = x_1 - x_2$, &emsp;  $Y_2 = Y_1  - x_2  - x_2$,  &emsp; $2x_2 = Y_1 - Y_2$,  &emsp; $x_2 = \frac{Y_1-Y_2}{2}$

Substituindo $x_2$ em $Y_1$

$Y_1 = x_1 + \frac{Y_1-Y_2}{2}$, &emsp; $x_1 = Y_1 - \frac{Y_1-Y_2}{2}$, &emsp; $x_1 = \frac{Y_1 + Y_2}{2}$

Assim obtemos duas novas equações <br><br>

Agora precisamos calcular o Jacobiano

 $J(x_1,x_2) = \begin{vmatrix}
\frac{\partial (x_1 + x_2)}{\partial x_1}&\frac{\partial (x_1 + x_2)}{\partial x_2}\\
\frac{\partial (x_1 - x_2)}{\partial x_1}&\frac{\partial (x_1 - x_2)}{\partial x_2}\\
\end{vmatrix} = 
\begin{vmatrix}
1&1\\
1&-1\\
\end{vmatrix} = -1 -1 = -2$
<br><br><br>
A equação definida anteriormente foi<br><br>

$f_{y_1, y_2}(y_1, y_2) = f(x_1, x_2) \cdot |J(x_1, x_2)|^{-1}$<br><br>

Na qual, $f_{y_1, y_2}(y_1, y_2)$ é uma exponencial como dado pelo exercício, lembrando que a fdp da distribuição é $f(x) = \theta e^{-\theta x}$

$f_{y_1, y_2} = \lambda_1 \lambda_2 \cdot e^{-\lambda_1 \left(\frac{y_1 + y_2}{2} \right)} \cdot e^{-\lambda_2 \left(\frac{y_1 - y_2}{2} \right)} |-2|^{-1} = \frac{\lambda_1 \lambda_2}{2} \cdot e^{-\lambda_1 \left(\frac{y_1 + y_2}{2} \right) -\lambda_2 \left(\frac{y_1 - y_2}{2} \right)}$

A última parte é definir o escopo

Os parâmetros da $f_{y_1, y_2}(y_1, y_2)$ são $y_1 + y_2$ e $y_1 - y_2$, e ela sendo uma exponencial, esses devem ser maiores que zero, assim $y_1 + y_2 > 0$ e $y_1 - y_2 > 0$.<br><br>

Além disso,

1 - $y_1 = x_1 + x_2$, portanto $y_1$ deve ser maior que zero.

2 - $\frac{y_1 + y_2}2 > 0$, &emsp; $y_1 + y_2 > 0$, &emsp; $y_2 > -y_1 $

3 - $\frac{y_1 - y_2}2 > 0$, &emsp; $y_1 - y_2 > 0$, &emsp; $y_1 >  y_2$

```{r, echo = FALSE}

curve(x*1, from=0, to= 12, xlab = 'y1', ylab = 'y2', xlim = c(0,10), ylim = c(-10,10), lwd = 1.5)
curve(x*-1, from=0, to= 12, xlab = 'y1', ylab = 'y2', lwd = 1.5, add=T)
text(8,6.8,  'y2=y1')
text(8,-6.8, 'y2=-y1')
```
<br><br>

**Passos para usar o método jacobiano**

1. Obter as inversas $x_1$ e $x_2$

2. Calcular o determinante do Jacobiano $J(x_1, x_2)$

3. Calcular a fdp de $y_1$ e $y_2$

4. Possíveis valores ou suporte de $y_1$ e $y_2$

<br><br><br>

### Função Geradora de Momentos - FGM

Se X for discreto,
$$M_X(t) = E(e^{tx}) = \sum_x e^{tx}I\!P(X=x)$$
Se X for contínuo,
$$M_X(t) = E(e^{tx}) = \int^{+\infty}_{-\infty}e^{tx}f(x)dx$$

No qual o primeiro momento é a média, o segundo momento pode ser usado para descobrir a variância, o terceiro para verificar assimetria e o quarto para calcular a curtose.

O r-ésimo momento de um v.a. X pode ser calculado derivando a fgm r vevez e avaliando no ponto t=0

$$E(x^r)=\left. \frac{d^r}{d^rt}m(t)\right|_{t=0} = m^{(r)}(0)$$
Propriedades:

- $Var(x) = m''(0) - (m'(0))^2$
- $M_{x+y}(t) = M_X(t) \cdot M_Y(t)$, se $X \perp Y$
- $X_1, X_2, ..., X_n$, n v.a's i.i.d. então $M_{x_1 + x_2 + ... + x_n}(t) = \prod_{i=1}^n m_{x_i}(t) = (m_{x_i}(t))^n$
- Unicidade: Se X e Y tem a mesma fgm então X e Y são identicamente distribuídas.
- Seja $Y = ax + b$ sua fgm será $M_Y(t) = M_{ax + b}(t) = e^{bt}m_x(at)$ <br><br><br>

### Função Característica <br><br>

A função característica de uma v.a X é definida por:

$$\varnothing_x(t) = I\!E(e^{itx}, t \in I\!R, i = \sqrt{-1})$$
Propriedades:

- $i^rE(x^r)=\left. \frac{d^r}{dt^r}\varnothing_x(t)\right|_{t=0}$

- Se $X \perp Y$ então $\varnothing_{x+y}(t) = \varnothing_x(t)\varnothing_y(t)$
 
- $\varnothing_{ax + b}(t) = e^{itb}\varnothing_x(at)$

- Unicidade, $\varnothing_{X}(t) = \varnothing_{Y}(t) <=>$ X e Y tem a mesma distribuição.<br><br>

### Desigualdade de Markov <br><br>

Seja X uma v.a positiva, então

$$P(x > t) \le \frac{E(x)}{t}, \forall t > 0$$
Demonstração (Caso contínuo):

$E(x) = \int^{\infty}_0 x f_x(x) dx = \int^t_0 x f_x(x) dx + \int^\infty_t xf_x(x)dx \ge \int^\infty_t tf_x(x)dx$

$E(x) = t P(x>t)$

logo, $I\!P(x>t) \le \frac {E(x)} t$<br><br> 

### Desigualdade de Chebyschew <br><br>

Seja x uma v.a tal que $Var(x)$ existe, então

$I\!P(|X-I\!E(x)| \ge t) \le \frac{Var(x)}{t^2}$<br><br>

Demonstração:  $Y = (X - E(X))^2$

$P(Y \ge 0) = 1$

$I\!E(Y)= E[ (x-E(x))^2]=Var(x)$

$I\!P(|X-E(x)| \ge t) = P(|X-E(x)|^2 \ge t^2) = P(Y \ge t^2) \le \frac{E(Y)}{t^2} = \frac{Var(x)}{t^2}$ 

&emsp;logo $P(|X-E(x)| \ge t) \le \frac{Var(x)}{t^2}$<br><br>

### Convergência em probabilidade<br><br>

Seja $x_1, x_2, ...$ uma sequência infinita de v.a's dizemos que essa sequência converge em probabilidade para $x_n \rightarrow^{I\!P} b, b \in I\!R$<br><br>

Se $\lim\limits_{n \to +\infty} I\!P(|x_n - x|) = 1, \forall$ &nbsp; $\varepsilon > 0$, então $\lim I\!P(|x_n - b| < \varepsilon) = 1, \forall$ &nbsp; $\varepsilon > 0$<br><br>

Ex) Seja $Z_1, Z_2, ..., Z_n$ v.a's tais que

$I\!P(Z_n = n^2) = \frac{1}n$ &emsp; $I\!P(Z_n = 0) = 1- \frac 1 n$

(*A distribuição é definida por duas equações, uma quando $Z_n é igual a zero e outra quando é diferente*)

Mostre que $Z_n \rightarrow^{I\!P} 0$<br><br>

$Z_n \rightarrow^{I\!P} 0 <=>_{n \rightarrow \infty} I\!P(|Z_n - 0| < \varepsilon) = 1$, &nbsp; $\forall$ &nbsp; $\varepsilon > 0$

(*Ou seja $Z_n$ converge em probabilidade para zero, se e somente, se, quando $n$ tende ao infinito e a probabilidade da diferença de $Z_n$ e o ponto 0 serem menor que $\varepsilon$ é igual a 1, sendo $\varepsilon$ um número muito pequeno. Que significa que, para valores grandes de $n$ $X_n$ e $X$ são iguais com probabilidade, aproximadamente, igual a 1.*)<br><br>

Se $0 < \varepsilon < n^2$,

$P(|Z_n - 0| < \varepsilon) = P(Z_n  < \varepsilon) = P(Z_n = 0) = 1 - \frac{1} n \rightarrow^{n \to \infty} 1$

(*Visto que $\varepsilon$ é um número muito pequeno, para $Z_n$ ser menor que ele, ele deve ser aproximadamente zero, a função de $P(Z_n = 0)$ foi definidada anteriormente, substituindo-a e aplicando o limite quando n tende ao infinito, chegamos que essa probabilidade é igual a 1.*)<br><br>

Se $\varepsilon > n^2$

$P(Z_n< \varepsilon) = P(Z_n = 0) + P(Z_n = n^2) = 1 - \frac 1 n + \frac 1 n = 1,$ &nbsp; $\forall$ &nbsp; $n$<br><br>

(*Somando a probabilidade das duas formas da distribuição por $\varepsilon$ ser maior que $n^2$, chegamos então que para todos os casos quando n tende ao infinito, a função tende a 1*)

Assim, $\lim\limits_{n \to \infty} P(|Z_n - 0| < \varepsilon) = 1$ &nbsp; $\forall$ &nbsp; $\varepsilon > 0$

Logo, $Z_n \rightarrow^{I\!P} 0$<br><br> 

### Lei Fraca dos Grandes Números<br><br>

$(X_n)n \ge 1$ sequência de v.a's iid, tais que

$E(X_n) = \mu, Var(x_n) \sigma^2 < \infty$

Seja $\bar x_n = \frac{\sum^n_{i=1}X_i}{n}$

$\bar{X_n} \rightarrow^p  \mu$<br><br>

### Lema de Borel-Continelli<br><br>

Seja $(A_n)_{n \ge 1}$ uma sequência de eventos

- Se $\sum^{\infty}_{n = 1} P(A_n) < \infty \rightarrow P(A_n$ infinitas vezes) = 0

- Se $A_i \perp A_j$ e $\sum^\infty_{n=1}P(A_n) = \infty \rightarrow  P(A_n$ infinitas vezes$)$ = 1<br><br>

Ex) Uma moeda honesta é lançada repetidamente e de forma independente, sejam

$A_n$: O n-ésimo lançamento resultou em "cara"

$B_n$: O n-ésimo e o (n+1)-ésimo lançamento resultaram em "cara"<br><br>

Mostre que: 

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; a) $I\!P(A_n i.v.) = 1$

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; b) $I\!P(B_n i.v.) = 1$ <br><br>


a ) $P(A_n) = \frac 1 2, n =1,2,...$

$P(A_n \cap A_{n+1}) = P(A_n)P(A_{n+1}), A_i \perp A_j$

$\sum^\infty_{n=1} P(A_n) = \sum^\infty_{n=1} \frac 1 2 = \infty$

Logo, pelo lema de B.C., $I\!P(A_n i.v) = 1$, ou seja se jogarmos infinitas vezes a moeda, aparecerá cara infinitas vezes com probabilidade igual 1.<br><br>

b ) $P(B_n) = P(A_n \cap A_{n+1}) = \frac 1 4, n = 1,2,...$

$P(B_1 \cap B_2) = P(A_1 \cap A_2 \cap A_2 \cap A_3) = P(A_1 \cap A_2 \cap A_3) = \left ( \frac 1 2 \right )^3 = \frac 1 8$

$P(B_1) = \frac 1 4 = P(B_2), P(B_1) \cdot P(B_2) = \frac 1 {16}$<br><br>

Assim, $B_1$ e $B_2$ não são independentes, ou seja não satisfaz o lema.
<br><br>
Definindo $C_n$ : Os lançamentos 2n e (2n - 1) resultaram em "cara"

$C_1 = A_1 \cap A_2 = B_1$

$C_2 = A_3 \cap A_4 = B_3$

$C_3 = A_5 \cap A_6 = B_5$


$C_n = B_{2n -1}$

$P(C_n) = \frac 1 4, \sum^\infty_{n=1} P(C_n) = \infty$

$P(C_n \cap C_{n+1}) = P(A_{2n-1} \cap A_{2n} \cap A_{2n+1} \cap A_{2n +2}) = \frac 1 {16} = P(C_n) \cdot P(C_{n+1})$<br><br>

Logo, por B.C., $P(C_n i.v.) = 1$

$P(B_n i.v.) = 1$<br><br><br>

### Convergência Quase-Certa<br><br>

Seja $(X_n)_{n\ge1}$ uma sequência de v.a's. Dizemos que $X_n$ converge quase certamente x, $X_n \rightarrow^{q_c}x$, se o evento

&emsp; &emsp; &emsp; $[w \in \Omega / X_n(w) \rightarrow^{n \rightarrow \infty} x]$ tem probabilidade 1.<br><br>

$X_n \rightarrow^{q_c} x$ se e somente se $P(|X_n - x| > \varepsilon_{iv}) = 0, \forall \varepsilon > 0$<br><br>

```{r, echo = F}
set.seed(999)
amostra <- rnorm(300)
n <- 1:300
medias <- sapply(n, FUN = function(x) { mean(sample(amostra, x, replace = T))})

plot(n, medias, type = 'l', ylab= '')

abline(h = mean(amostra), col = 'green', lwd =1.2)
abline(h = mean(amostra)+ 0.09, col = 'red', lty = "dashed")
abline(h = mean(amostra)- 0.09, col = 'red', lty = "dashed")

text(65, 0.3, labels = '(|Xn - X|)', cex= .75)

text(270, mean(amostra)- 0.19, labels = '- epsilon', col = 'red', cex= .75)
text(270, mean(amostra)+ 0.19, labels = '+ epsilon', col = 'red', cex = 0.75)

```
<p>Ou seja, quando $n$ tende ao infinito, $X_n - X$ ficará entre $-\varepsilon$ e  $+\varepsilon$ com probabilidade 1, e só daí $X_n \to^{q_c} X$.</p> 


Logo, por Borel-Contelli, temos que se $\sum^\infty_{n=1} P(|X_n - x| > \varepsilon) < \infty$ 

<center>então $X_n \rightarrow^{q_c}x$</center><br><br>

Ex) $(X_n)_{n \ge 1}$ uma sequência de va's independentes Bernoulli$\left ( \frac 1 {n^2} \right )$, mostre que $X_n \to^{q_c}0$

$P(|X_n - 0| > \varepsilon$ &nbsp; $i.v) = 0, \forall$&nbsp;$\epsilon > 0$


\begin{equation} 
 P(X_n > \varepsilon) = \begin{cases}
      0 \ \ \ , \  \varepsilon\ge 1
      \\
      \frac{1}{n^2}  \ , \ 0 < \varepsilon < 1
    \end{cases} 
    \end{equation}
    
$\sum^\infty_{n=1} P(|X_n - 0| > \varepsilon)$
    
\begin{equation} 
 \sum^\infty_{n=1}(X_n > \varepsilon) = \begin{cases}
      \sum^\infty_{n=1} 0 \ \ \ = \  0
      \\
      \sum^\infty_{n=1}\frac{1}{n^2}  \ =  \frac {\pi^2} 6 \ , \ 0 < \varepsilon < 1
    \end{cases} 
    \end{equation}

$\sum P < \infty, \forall$&nbsp; $\varepsilon > 0$

Por B.C, $P(|X_n - 0| > \varepsilon_{i.v}) = 0$

Logo, $X_n \to^{q_c} 0$<br><br>
    
### Lei Forte dos Grandes números <br><br>

Sejam $(X_n)_{n \ge 1}$ v.a's iid com média finita $\mu$ e $\bar X_n = \frac{x_1 + x_2 + ... + x_n} n$, então

<center>;  $\bar X_n \rightarrow^{q_c} \mu$</center><br><br>

### Convergência em Distribuição <br><br>

Sejam $(X_n)_{n \ge 1}$ e X v.a's com fda $F_n(x)$ e $F(x)$, respectivamente, dizemos que $X_n$ converge em distribuição para x, 

<center>$X_n \rightarrow^D x$ se $\lim\limits_{n \to \infty} F_n(x) = F(x)$</center> <br><br>

### Convergência<br><br>

- $X_n \rightarrow^{I\!P}x$, se $\lim\limits_{n \to +\infty} I\!P(|x_n - x|) = 1, \forall \varepsilon > 0$

- $X_n \rightarrow^{q_c}x$, se $P(|X_n - x| > \varepsilon_{iv}) = 1, \forall \varepsilon > 0$

- $X_n \rightarrow^D x$, se $\lim\limits_{n \to \infty} F_n(x) = F_x(x)$<br><br>

### Teorema de Slustsky<br><br>

Se $X_n \rightarrow^D X, Y_n \rightarrow^D C, C \in I\!R$

- $X_n Y_n \rightarrow^D CX$

- $X_n \pm Y_n \rightarrow^D X\pm C$

- $\frac{X_n}{Y_n} \rightarrow^D \frac X C, C \neq 0$<br><br>

### Teorema do Limite Central<br><br>

Sejam $(X_n)_{n \ge1}$ v.a's  iid com média $\mu$ e variância $\sigma^2 < \infty$. Seja $\bar X_n = \frac{x_1 +...+x_n}{n}$,então 

<center>  $\frac{\bar{X_n} - \mu}{\frac{\sigma}{\sqrt n}} \rightarrow^D N(0,1)$ </center>

<br><br><br>

## Calculo IV

## Métodos Numéricos

## Estatística Computacional

## Inferência

